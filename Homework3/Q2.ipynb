{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointNet with Input and Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9840, 2048, 3)\n",
      "(9840, 1)\n"
     ]
    }
   ],
   "source": [
    "# load training data and labels\n",
    "data0 = utils.load_h5(\"ply_data_train0.h5\")\n",
    "data1 = utils.load_h5(\"ply_data_train1.h5\")\n",
    "data2 = utils.load_h5(\"ply_data_train2.h5\")\n",
    "data3 = utils.load_h5(\"ply_data_train3.h5\")\n",
    "data4 = utils.load_h5(\"ply_data_train4.h5\")\n",
    "\n",
    "# aggregate training data, training label\n",
    "train_data = np.append(data0[0], data1[0], axis=0)\n",
    "train_data = np.append(train_data, data2[0], axis=0)\n",
    "train_data = np.append(train_data, data3[0], axis=0)\n",
    "train_data = np.append(train_data, data4[0], axis=0)\n",
    "print(np.shape(train_data))\n",
    "\n",
    "train_labels = np.append(data0[1], data1[1], axis=0)\n",
    "train_labels = np.append(train_labels, data2[1], axis=0)\n",
    "train_labels = np.append(train_labels, data3[1], axis=0)\n",
    "train_labels = np.append(train_labels, data4[1], axis=0)\n",
    "print(np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = utils.load_h5(\"ply_data_test0.h5\")\n",
    "test1 = utils.load_h5(\"ply_data_test1.h5\")\n",
    "\n",
    "test_data = np.append(test0[0], test1[0], axis=0)\n",
    "test_labels = np.append(test0[1], test1[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_one_hot = []\n",
    "for l in train_labels:\n",
    "    one_hot = np.zeros(40, dtype=np.int)\n",
    "    one_hot[l[0]] = 1\n",
    "    train_labels_one_hot.append(one_hot)\n",
    "train_labels_one_hot = np.array(train_labels_one_hot)\n",
    "\n",
    "# one hot encode test_labels\n",
    "test_labels_one_hot = []\n",
    "for l in test_labels:\n",
    "    one_hot = np.zeros(40, dtype=np.int)\n",
    "    one_hot[l[0]] = 1\n",
    "    test_labels_one_hot.append(one_hot)\n",
    "test_labels_one_hot = np.array(test_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(cloud):\n",
    "    # B x 2048 x 3 x 1\n",
    "    batch_norm = tf.contrib.layers.batch_norm\n",
    "    # 1st mlp layer\n",
    "    i_trans_layer_conv1 = tf.contrib.layers.conv2d(inputs=cloud, num_outputs=64, kernel_size=[1, 3], padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "    # (B, 2048, 64)\n",
    "\n",
    "    # 2nd mlp layer\n",
    "    i_trans_layer_conv2 = tf.contrib.layers.conv2d(inputs=i_trans_layer_conv1, num_outputs=128, kernel_size=[1, 1], padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "    # (B, 2048, 64)\n",
    "\n",
    "    # 3rd mlp layer\n",
    "    i_trans_layer_conv3 = tf.contrib.layers.conv2d(inputs=i_trans_layer_conv2, num_outputs=1024, kernel_size=[1, 1], padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "    # (B, 2048, 64)\n",
    "\n",
    "    # pooling\n",
    "    i_max_pool = tf.contrib.layers.max_pool2d(inputs=i_trans_layer_conv3, kernel_size=[2048, 1], stride=1, padding=\"VALID\")\n",
    "\n",
    "    i_max_pool = tf.reshape(i_max_pool, [batch_size, -1])\n",
    "    \n",
    "    # fnn1\n",
    "    i_layer_fnn1 = tf.contrib.layers.fully_connected(inputs=i_max_pool, num_outputs=512, activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "\n",
    "    # fnn2\n",
    "    i_layer_fnn2 = tf.contrib.layers.fully_connected(inputs=i_layer_fnn1, num_outputs=256, activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "\n",
    "    initial = np.eye(3).flatten()\n",
    "    transform = tf.contrib.layers.fully_connected(inputs=i_layer_fnn2, num_outputs = 3*3, biases_initializer=tf.constant_initializer(initial))\n",
    "\n",
    "    return tf.reshape(transform, (-1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(32, 2048, 3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "cloud_origin = tf.placeholder(tf.float32, [batch_size, 2048, 3])\n",
    "cloud = tf.reshape(cloud_origin, (-1, 2048, 3, 1))\n",
    "print(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply input transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 3)\n",
      "Tensor(\"Reshape_3:0\", shape=(32, 2048, 3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# apply input transformation\n",
    "t = input_transform(cloud)\n",
    "print(np.shape(t))\n",
    "transformed_input = tf.reshape(tf.matmul(cloud_origin, t), (-1, 2048, 3, 1))\n",
    "print(transformed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform(net):\n",
    "    # B x 2048 x 3 x 1\n",
    "\n",
    "    # 1st mlp layer\n",
    "    f_trans_layer_conv1 = tf.contrib.layers.conv2d(inputs=net, num_outputs=64, kernel_size=1, padding=\"VALID\", activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    # (B, 2048, 1, 64)\n",
    "\n",
    "    # 2nd mlp layer\n",
    "    f_trans_layer_conv2 = tf.contrib.layers.conv2d(inputs=f_trans_layer_conv1, num_outputs=128, kernel_size=1, padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "    # (B, 2048, 1, 64)\n",
    "\n",
    "    # 3rd mlp layer\n",
    "    f_trans_layer_conv3 = tf.contrib.layers.conv2d(inputs=f_trans_layer_conv2, num_outputs=1024, kernel_size=1, padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "    # (B, 2048, 1, 64)\n",
    "\n",
    "    # pooling\n",
    "    f_max_pool = tf.contrib.layers.max_pool2d(inputs=f_trans_layer_conv3, kernel_size=[2048, 1], stride=1, padding=\"VALID\")\n",
    "\n",
    "    f_max_pool = tf.reshape(f_max_pool, [batch_size, -1])\n",
    "    \n",
    "    # fnn1\n",
    "    f_layer_fnn1 = tf.contrib.layers.fully_connected(inputs=f_max_pool, num_outputs=512, activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "\n",
    "    # fnn2\n",
    "    f_layer_fnn2 = tf.contrib.layers.fully_connected(inputs=f_layer_fnn1, num_outputs=256, activation_fn=tf.nn.relu, normalizer_fn=batch_norm)\n",
    "\n",
    "    K = 64\n",
    "    initial = np.eye(K).flatten()\n",
    "    transform = tf.contrib.layers.fully_connected(inputs=f_layer_fnn2, num_outputs = K*K, biases_initializer=tf.constant_initializer(initial))\n",
    "\n",
    "    return tf.reshape(transform, (-1, K, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main network, apply feature transformation after conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main network\n",
    "# placeholder for one-hot labels\n",
    "y = tf.placeholder(tf.float32, [None, 40])\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "decay_rate = tf.placeholder(tf.float32, shape=[])\n",
    "batch_norm = tf.contrib.layers.batch_norm\n",
    "\n",
    "# placeholder for labels\n",
    "y_labels = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# 1st mlp layer\n",
    "layer_conv1 = tf.contrib.layers.conv2d(inputs=transformed_input, num_outputs=64, kernel_size=[1, 3], padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "\n",
    "# 2nd mlp layer\n",
    "layer_conv2 = tf.contrib.layers.conv2d(inputs=layer_conv1, num_outputs=64, kernel_size=1, padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "\n",
    "# apply feature transformation\n",
    "f_transform = feature_transform(layer_conv2)\n",
    "layer_conv2 = tf.matmul(tf.squeeze(layer_conv2, [2]), f_transform)\n",
    "layer_conv2 = tf.expand_dims(layer_conv2, [2])\n",
    "\n",
    "# 3rd mlp layer\n",
    "layer_conv3 = tf.contrib.layers.conv2d(inputs=layer_conv2, num_outputs=64, kernel_size=1, padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "# 4th cnn\n",
    "layer_conv4 = tf.contrib.layers.conv2d(inputs=layer_conv3, num_outputs=128, kernel_size=[1, 1], padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "# 5th cnn\n",
    "layer_conv5 = tf.contrib.layers.conv2d(inputs=layer_conv4, num_outputs=1024, kernel_size=[1, 1], padding=\"VALID\", activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "# max pooling\n",
    "max_pool = tf.contrib.layers.max_pool2d(inputs=layer_conv5, kernel_size=[2048, 1], stride = 1, padding=\"VALID\")\n",
    "\n",
    "# fnn1\n",
    "layer_fnn1 = tf.contrib.layers.fully_connected(inputs=max_pool, num_outputs=512, activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "# fnn2\n",
    "layer_fnn2 = tf.contrib.layers.fully_connected(inputs=layer_fnn1, num_outputs=256, activation_fn=tf.nn.relu, normalizer_fn=batch_norm, normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "layer_fnn2 = tf.contrib.layers.dropout(inputs=layer_fnn2, keep_prob=0.7)\n",
    "\n",
    "# fnn3\n",
    "logits = tf.contrib.layers.fully_connected(inputs=layer_fnn2, num_outputs=40, activation_fn=tf.nn.relu)\n",
    "logits = tf.squeeze(logits, [1, 2])\n",
    "\n",
    "# softmax\n",
    "output = tf.nn.softmax(logits)\n",
    "output_class = tf.reshape(output, (-1,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import math\n",
    "# only rotate against y axis\n",
    "def rotate(pt_cloud):\n",
    "    angle = np.deg2rad(randint(0, 360))\n",
    "    R = np.array([[math.cos(angle), 0, math.sin(angle)], [0, 1, 0], [-math.sin(angle), 0, math.cos(angle)]])\n",
    "    rotated_pt_cloud = np.matmul(R, pt_cloud.T).T\n",
    "    return rotated_pt_cloud\n",
    "\n",
    "def jitter(pt_cloud):\n",
    "    return pt_cloud + np.random.normal(0, 0.02, None)\n",
    "\n",
    "def augment(pt_cloud):\n",
    "    seed = np.random.randint(10)\n",
    "    if seed == 0:\n",
    "        return rotate(jitter(pt_cloud))\n",
    "    elif seed == 1:\n",
    "        return rotate(pt_cloud)\n",
    "    elif seed == 2:\n",
    "        return jitter(pt_cloud)\n",
    "    else:\n",
    "        return pt_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    right_count = 0\n",
    "    i = 0\n",
    "    while i < len(test_data):\n",
    "        j = min(i + 32, len(test_data))\n",
    "        correct_labels = tf.equal(tf.argmax(output_class, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_labels, tf.float32))\n",
    "    # compute accuracy on test data\n",
    "        labels = np.array([label.argmax() for label in test_labels_one_hot[i:j]])\n",
    "    #print(np.shape(labels))\n",
    "        accuracy = sess.run([accuracy],feed_dict = {cloud_origin: test_data[i:j], y: test_labels_one_hot[i:j]})\n",
    "        right_count = right_count + accuracy[0] * (j - i)\n",
    "        i += 32\n",
    "    final_accuracy = right_count / len(test_data)\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "l_rate = 0.001\n",
    "dr = 0.5\n",
    "\n",
    "epoch = 200\n",
    "num_iter = 307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, manually terminated after 170 epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10; loss = 1.30526; accuracy = 0.71875; lr = 0.0010000000474974513\n",
      "epoch 20; loss = 0.758977; accuracy = 0.752435064935; lr = 0.0010000000474974513\n",
      "epoch 30; loss = 0.834988; accuracy = 0.79586038961; lr = 0.0005000000237487257\n",
      "epoch 40; loss = 0.975209; accuracy = 0.798701298701; lr = 0.0005000000237487257\n",
      "epoch 50; loss = 0.675526; accuracy = 0.826298701299; lr = 0.0002500000118743628\n",
      "epoch 60; loss = 0.596541; accuracy = 0.821022727273; lr = 0.0002500000118743628\n",
      "epoch 70; loss = 0.415493; accuracy = 0.836444805195; lr = 0.0001250000059371814\n",
      "epoch 80; loss = 0.422969; accuracy = 0.843344155844; lr = 0.0001250000059371814\n",
      "epoch 90; loss = 0.721235; accuracy = 0.842938311688; lr = 6.25000029685907e-05\n",
      "epoch 100; loss = 0.575737; accuracy = 0.840097402597; lr = 6.25000029685907e-05\n",
      "epoch 110; loss = 0.50492; accuracy = 0.845779220779; lr = 3.125000148429535e-05\n",
      "epoch 120; loss = 0.426225; accuracy = 0.846996753247; lr = 3.125000148429535e-05\n",
      "epoch 130; loss = 0.494703; accuracy = 0.844967532468; lr = 1.5625000742147677e-05\n",
      "epoch 140; loss = 0.413442; accuracy = 0.852272727273; lr = 1.5625000742147677e-05\n",
      "epoch 150; loss = 0.662088; accuracy = 0.849837662338; lr = 7.812500371073838e-06\n",
      "epoch 160; loss = 0.541926; accuracy = 0.848214285714; lr = 7.812500371073838e-06\n",
      "epoch 170; loss = 0.487681; accuracy = 0.844967532468; lr = 3.906250185536919e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3be24738fafd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         batch_img_vanilla = train_data[idx][:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         batch_y = train_labels_one_hot[idx][:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcloud_origin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_img_vanilla\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = np.array([augment(x) for x in train_data])\n",
    "for e in range(epoch):\n",
    "    if dr < 0.99:\n",
    "        dr += 0.01\n",
    "    for i in range(num_iter):\n",
    "        idx = np.random.choice(9840, [batch_size], False)\n",
    "        batch_img_vanilla = train_data[idx][:]\n",
    "        batch_y = train_labels_one_hot[idx][:]\n",
    "        _, l, lrate = sess.run([optimizer, loss, optim._lr],feed_dict = {cloud_origin: batch_img_vanilla , y: batch_y, learning_rate: l_rate, decay_rate : dr})\n",
    "    if (e + 1) % 10 == 0:\n",
    "        ax = get_accuracy()\n",
    "        print (\"epoch \" + str(e+1) + \"; loss = \" + str(l) + \"; accuracy = \" + str(ax) + \"; lr = \" + str(lrate))\n",
    "    if e % 20 == 0 and e > 0:\n",
    "        l_rate /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy 0.852272727273\n"
     ]
    }
   ],
   "source": [
    "print(\"Final accuracy\", get_accuracy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
