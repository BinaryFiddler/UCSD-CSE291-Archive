{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import h5py\n",
    "import utils\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNET40_PATH = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(h5_filename):\n",
    "    \"\"\"\n",
    "    Data loader function.\n",
    "    Input: The path of h5 filename\n",
    "    Output: A tuple of (data,label)\n",
    "    \"\"\"\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_names():\n",
    "    \"\"\"\n",
    "    Function to list out all the categories in MODELNET40\n",
    "    \"\"\"\n",
    "    shape_names_file = os.path.join(MODELNET40_PATH, 'shape_names.txt')\n",
    "    shape_names = [line.rstrip() for line in open(shape_names_file)]\n",
    "    return shape_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_labels,predicted_labels):\n",
    "    \"\"\"\n",
    "    Function to calculate the total accuracy.\n",
    "    Input: The ground truth labels and the predicted labels\n",
    "    Output: The accuracy of the model\n",
    "    \"\"\"\n",
    "    return np.mean(true_labels == predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "X_train = []\n",
    "y_temp = []\n",
    "for i in range(5):\n",
    "    fn = \"ply_data_train\" + str(i)+\".h5\"\n",
    "    h5_data = load_h5(fn)\n",
    "    for cloud in h5_data[0]:\n",
    "        X_train.append(cloud)\n",
    "    for label in h5_data[1]:\n",
    "        y_temp.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(deg, cloud):\n",
    "    rad = np.deg2rad(deg)\n",
    "    c = np.cos(rad)\n",
    "    s = np.sin(rad)\n",
    "    rotmat = np.array([[c, 0 ,s],\n",
    "                       [0, 1, 0],\n",
    "                       [-s, 0, c]])\n",
    "    cloud = np.matmul(rotmat, cloud.T).T\n",
    "    return cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jittering(cloud):\n",
    "    return cloud + np.random.normal(loc=0.0, scale=0.02, size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(cloud):\n",
    "    \n",
    "    seed = np.random.randint(10)\n",
    "    if seed == 0:\n",
    "        return rotate(np.random.randint(360),jittering(cloud))\n",
    "    elif seed == 1:\n",
    "        return rotate(np.random.randint(360), cloud)\n",
    "    elif seed == 2:\n",
    "        return jittering(cloud)\n",
    "    else:\n",
    "        return cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9840, 2048, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the dimension of the h5 data is correct\n",
    "X_train = np.array(X_train)\n",
    "X_train = np.array([aug(x) for x in X_train])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9840, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_temp = np.reshape(y_temp, (-1, 1))\n",
    "y_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros((y_temp.shape[0], K))\n",
    "for i in range(len(y_temp)):\n",
    "    y_train[i][y_temp[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_temp = []\n",
    "for i in range(2):\n",
    "    fn = \"ply_data_test\" + str(i)+\".h5\"\n",
    "    h5_data = load_h5(fn)\n",
    "    for cloud in h5_data[0]:\n",
    "        X_test.append(cloud)\n",
    "    for label in h5_data[1]:\n",
    "        y_temp.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_temp = np.reshape(y_temp, (-1, 1))\n",
    "y_test = np.zeros((y_temp.shape[0], K))\n",
    "for i in range(len(y_temp)):\n",
    "    y_test[i][y_temp[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, y, batch_size):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    indices = np.arange(0 , len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    selected = indices[:batch_size]\n",
    "#     print(selected)\n",
    "    X_ = [X[s] for s in selected]\n",
    "    y_ = [y[s] for s in selected]\n",
    "\n",
    "    return np.asarray(X_), np.asarray(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tNet(inputs, output_dim):\n",
    "    batch_norm = tf.contrib.layers.batch_norm\n",
    "    if output_dim == 3:\n",
    "        mlp1 = tf.contrib.layers.conv2d(inputs=inputs, num_outputs=64, kernel_size=[1,3], padding=\"VALID\",\n",
    "                                    activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    else:\n",
    "        mlp1 = tf.contrib.layers.conv2d(inputs=inputs, num_outputs=64, kernel_size=1, padding=\"VALID\",\n",
    "                                    activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    mlp2 = tf.contrib.layers.conv2d(inputs=mlp1, num_outputs=128, kernel_size=1, padding=\"VALID\",\n",
    "                                    activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    mlp3 = tf.contrib.layers.conv2d(inputs=mlp2, num_outputs=1024, kernel_size=1, padding=\"VALID\",\n",
    "                                    activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    global_feature = tf.contrib.layers.max_pool2d(inputs = mlp3, kernel_size=[2048,1], stride = 1, padding=\"VALID\")\n",
    "    fc1 = tf.contrib.layers.fully_connected(inputs=global_feature, num_outputs=512, \n",
    "                                        activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=256, \n",
    "                                        activation_fn=tf.nn.relu,normalizer_fn=batch_norm)\n",
    "    init = np.eye(output_dim).flatten()\n",
    "    output = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs = output_dim*output_dim, \n",
    "                                              biases_initializer=tf.constant_initializer(init))\n",
    "    return tf.reshape(output, (-1, output_dim, output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Input layer\n",
    "batch_norm = tf.contrib.layers.batch_norm\n",
    "X = tf.placeholder(tf.float32, shape = (None, 2048, 3))\n",
    "y_ = tf.placeholder(tf.float32, shape = (None, 40))\n",
    "\n",
    "X_in = tf.reshape(X, (-1, 2048, 3, 1))\n",
    "t1 = tNet(X_in, 3)\n",
    "X_transformed = tf.reshape(tf.matmul(X, t1), (-1, 2048, 3, 1))\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "decay_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "mlp1 = tf.contrib.layers.conv2d(inputs=X_transformed, num_outputs=64, kernel_size=[1,3], padding=\"VALID\",\n",
    "                                activation_fn=tf.nn.relu, \n",
    "                                normalizer_fn=batch_norm, \n",
    "                                normalizer_params = {'decay':decay_rate})\n",
    "\n",
    "mlp2 = tf.contrib.layers.conv2d(inputs=mlp1, num_outputs=64, kernel_size=1, padding=\"VALID\",\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                normalizer_fn=batch_norm,\n",
    "                                normalizer_params = {'decay':decay_rate})\n",
    "t2 = tNet(mlp2, 64)\n",
    "mlp2_transformed = tf.expand_dims(tf.matmul(tf.squeeze(mlp2, [2]), t2), [2])\n",
    "\n",
    "\n",
    "mlp3 = tf.contrib.layers.conv2d(inputs=mlp2_transformed , num_outputs=64, kernel_size=1, padding=\"VALID\",\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                normalizer_fn=batch_norm,\n",
    "                                normalizer_params = {'decay':decay_rate})\n",
    "mlp4 = tf.contrib.layers.conv2d(inputs=mlp3, num_outputs=128, kernel_size=1, padding=\"VALID\",\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                normalizer_fn=batch_norm,\n",
    "                                normalizer_params = {'decay':decay_rate})\n",
    "mlp5 = tf.contrib.layers.conv2d(inputs=mlp4, num_outputs=1024, kernel_size=1, padding=\"VALID\",\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                normalizer_fn=batch_norm,\n",
    "                                normalizer_params = {'decay':decay_rate})\n",
    "# Pooling layer\n",
    "global_feature = tf.contrib.layers.max_pool2d(inputs = mlp5, kernel_size=[2048,1], stride = 1, padding=\"VALID\")\n",
    "\n",
    "fc1 = tf.contrib.layers.fully_connected(inputs=global_feature, num_outputs=512, \n",
    "                                        activation_fn=tf.nn.relu,\n",
    "                                        normalizer_fn=batch_norm,\n",
    "                                        normalizer_params = {'decay':decay_rate})\n",
    "fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=256, \n",
    "                                        activation_fn=tf.nn.relu,\n",
    "                                        normalizer_fn=batch_norm,\n",
    "                                        normalizer_params = {'decay':decay_rate})\n",
    "# Apply a dropout operation with keep ratio of 0.7\n",
    "fc2 = tf.contrib.layers.dropout(fc2, keep_prob=0.7)\n",
    "logits = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=K, \n",
    "                                           activation_fn=tf.nn.relu,\n",
    "                                           normalizer_fn=batch_norm,\n",
    "                                           normalizer_params = {'decay':decay_rate})\n",
    "y = tf.nn.softmax(logits)\n",
    "y = tf.reshape(y, (-1,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    lr = 0.001 # learning rate initialization\n",
    "    dr = 0.50 # decay rate for batch_norm initialization\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = logits))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optim = optimizer.minimize(loss)\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    #Make batches to train\n",
    "    epochs = 100\n",
    "    num_iter = 307\n",
    "    batch_size = 32\n",
    "    for e in range(epochs):\n",
    "        l = 0\n",
    "        if (dr < 0.99): \n",
    "            dr += 0.01\n",
    "        for i in range(num_iter):\n",
    "            batch_X, batch_y = next_batch(X_train, y_train, batch_size)\n",
    "            _, l, lrate = sess.run([optim, loss, optimizer._lr],feed_dict = {X: batch_X , y_: batch_y, \n",
    "                                                       learning_rate: lr, decay_rate : dr})\n",
    "        \n",
    "        if (e+1) % 10 == 0:\n",
    "            correct_labels = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_labels, tf.float32))\n",
    "            acc = []\n",
    "            i = 0\n",
    "            # Calculate the result from the test set\n",
    "            # Patched due to the limitation of GPU memory\n",
    "            while((i+1) * batch_size < len(X_test)):\n",
    "                acc.append(sess.run(accuracy, feed_dict={X: X_test[i*batch_size:(i+1)*batch_size],\n",
    "                                                         y_: y_test[i*batch_size:(i+1)*batch_size]}))\n",
    "                i+=1\n",
    "            ax = sum(acc) / len(acc)\n",
    "            remain = sess.run(accuracy, feed_dict={X: X_test[i*batch_size:],y_: y_test[i*batch_size:]})\n",
    "            rlen = len(X_test[i*batch_size:])\n",
    "            ax = ax * (len(X_test)-rlen) / len(X_test) + remain * rlen / len(X_test)\n",
    "            print (\"epoch \" + str(e+1) + \"; loss = \" + str(l) + \"; accuracy = \" + str(ax)\n",
    "                   + \"; lr = \" + str(lrate))\n",
    "        if e % 20 == 0 and e > 0:\n",
    "            lr /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10; loss = 0.965813; accuracy = 0.695705024311; lr = 0.0010000000474974513\n",
      "epoch 20; loss = 1.06671; accuracy = 0.726499189627; lr = 0.0010000000474974513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-2b7221ef68a6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             _, l, lrate = sess.run([optim, loss, optimizer._lr],feed_dict = {X: batch_X , y_: batch_y, \n\u001b[0;32m---> 20\u001b[0;31m                                                        learning_rate: lr, decay_rate : dr})\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
